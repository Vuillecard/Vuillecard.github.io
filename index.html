<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Portfolio of Pierre Vuillecard - Research Scientist">
    <title> Pierre Vuillecard </title>
    <link href="https://fonts.googleapis.com/css2?family=Helvetica:wght@300;400;700&display=swap" rel="stylesheet">
    <style>
        body {
            margin: 0;
            font-family: 'Helvetica', sans-serif;
            background-color: #f5f5f7;
            color: #333;
            line-height: 1.6;
        }
        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 50px 20px;
            background-color: #fff;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            max-width: 1200px;
            margin: 0 auto;
        }
        .header-content {
            max-width: 600px;
        }
        .header-content h1 {
            margin: 0;
            font-size: 2.5rem;
            font-weight: 700;
        }
        .header-content p {
            margin: 10px 0;
            font-size: 1.2rem;
            font-weight: 300;
            color: #555;
        }
        .header-photo {
            flex-shrink: 0;
            width: 150px;
            height: 150px;
            border-radius: 50%;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        .header-photo img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        section {
            max-width: 1000px;
            margin: 40px auto;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
        }
        section h2 {
            font-size: 1.8rem;
            font-weight: 700;
            margin-bottom: 20px;
            color: #1d1d1f;
        }
        .contact {
            text-align: center;
            font-size: 1rem;
            margin-bottom: 40px;
        }
        .contact a {
            text-decoration: none;
            color: #007aff;
        }
        .projects {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        .project {
            display: flex;
            gap: 20px;
            align-items: flex-start;
            background-color: #f9f9f9;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
            padding: 15px;
        }
        .project img {
            flex: 0 0 250px;
            width: 250px;
            height: 150px;
            object-fit: cover;
            border-radius: 8px;
        }
        .project-content {
            flex: 1;
        }
        .project h3 {
            font-size: 1.4rem;
            margin: 10px 0;
            font-weight: 700;
            color: #1d1d1f;
        }
        .project p {
            font-size: 1rem;
            margin-bottom: 10px;
            color: #555;
        }
        .project a {
            text-decoration: none;
            color: #007aff;
            font-weight: 500;
        }
        footer {
            text-align: center;
            padding: 20px 10px;
            font-size: 0.9rem;
            color: #777;
            background-color: #f5f5f7;
        }
    </style>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Pierre Vuillecard</h1>
            <p>PhD student | EPFL | IDIAP</p>
        </div>
        <div class="header-photo">
            <img src="https://raw.githubusercontent.com/Vuillecard/Vuillecard.github.io/refs/heads/main/images/profile_picture.jpeg" alt="[Your Name]'s photo">
        </div>
    </header>
    <section>
        <h3>About Me</h3>
        <p>I am a PhD student at EPFL (École polytechnique fédérale de Lausanne) & Idiap Research Institute in Switzerland, where I work on computer vision, deep learning, and human-centric scene understanding under the supervision of Jean-Marc Odobez.</p>
    </section>
    <section>
        <h3> Research Projects</h3>
        <div class="projects">
            <div class="project">
                <img src="https://raw.githubusercontent.com/Vuillecard/Vuillecard.github.io/refs/heads/main/images/gazevlm.png" alt="Project 1">
                <div class="project-content">
                    <!-- <h3>Project 1</h3> -->
                    <a href="https://project2-link.com" target="_blank">A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction</a>
                    <p>Anshul Gupta, Samy Tafasca, Arya Farkhondeh, Pierre Vuillecard, Jean-Marc Odobez</p>
                    <p>We investigate the zero-shot capabilities of Vision-Language Models (VLMs) for extracting a wide array of contextual cues to improve gaze following performance. We first evaluate various VLMs, prompting strategies, and in-context learning (ICL) techniques for zero-shot cue recognition performance. Our analysis indicates that BLIP-2 is the overall top performing VLM and that ICL can improve performance. For gaze following, incorporating the extracted cues results in better generalization performance, especially when considering a larger set of cues.</p>
                </div>
            </div>
            <div class="project">
                <img src="https://raw.githubusercontent.com/Vuillecard/Vuillecard.github.io/refs/heads/main/images/gazevlm.png" alt="Project 1">
                <div class="project-content">
                    <!-- <h3>Project 1</h3> -->
                    <a href="https://project2-link.com" target="_blank">Exploring the Zero-Shot Capabilities of Vision-Language Models for Improving Gaze Following</a>
                    <p>We investigate the zero-shot capabilities of Vision-Language Models (VLMs) for extracting a wide array of contextual cues to improve gaze following performance. We first evaluate various VLMs, prompting strategies, and in-context learning (ICL) techniques for zero-shot cue recognition performance. Our analysis indicates that BLIP-2 is the overall top performing VLM and that ICL can improve performance. For gaze following, incorporating the extracted cues results in better generalization performance, especially when considering a larger set of cues.</p>
                </div>
            </div>
            <div class="project">
                <img src="https://raw.githubusercontent.com/Vuillecard/Vuillecard.github.io/refs/heads/main/images/gazevlm.png" alt="Project 1">
                <div class="project-content">
                    <!-- <h3>Project 1</h3> -->
                    <a href="https://project2-link.com" target="_blank">Real-time Head Gesture Recognition Demo</a>
                    <p>Developed a WebApp that transform your head gesture into gif and display them. It can handle up to 4 people at the same time.</p>
                </div>
            </div>
            <div class="project">
                <img src="https://raw.githubusercontent.com/Vuillecard/Vuillecard.github.io/refs/heads/main/images/All_gestures.png" alt="Project 2">
                <div class="project-content">
                    <!-- <h3>Project 2</h3> -->
                    <a href="https://project2-link.com" target="_blank">CCDb-HG: Novel Annotations and Gaze-Aware Representations for Head Gesture Recognition</a>
                    <p>CCDb-HG is a novel and large dataset for head gesture recognition, 
                        featuring diverse head gesture categories. Furthermore, we explore the inclusion of gaze as an auxiliary cue, 
                        along with geometric and temporal data augmentation, to enhance generalization. Additionally, we evaluate various model 
                        architectures to establish baseline performance on CCDb-HG.</p>
                </div>
            </div>
            <!-- Add more project cards as needed -->
        </div>
    </section>
    <footer>
        &copy; 2024 Pierre Vuillecard. All Rights Reserved.
    </footer>
</body>
</html>
