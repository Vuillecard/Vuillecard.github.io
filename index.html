<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Portfolio of Pierre Vuillecard - Research Scientist">
    <title> Pierre Vuillecard </title>
    <link href="https://fonts.googleapis.com/css2?family=Helvetica:wght@300;400;700&display=swap" rel="stylesheet">
    <style>
        body {
            margin: 0;
            font-family: 'Helvetica', sans-serif;
            background-color: #f5f5f7;
            color: #333;
            line-height: 1.6;
        }
        header {
            display: flex;
            justify-content: space-between;
            align-items: center;
            padding: 50px 20px;
            background-color: #fff;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            max-width: 1200px;
            margin: 0 auto;
        }
        .header-content {
            max-width: 600px;
        }
        .header-content h1 {
            margin: 0;
            font-size: 2.5rem;
            font-weight: 700;
        }
        .header-content p {
            margin: 10px 0;
            font-size: 1.2rem;
            font-weight: 300;
            color: #555;
        }
        .header-photo {
            flex-shrink: 0;
            width: 150px;
            height: 150px;
            border-radius: 50%;
            overflow: hidden;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }
        .header-photo img {
            width: 100%;
            height: 100%;
            object-fit: cover;
        }
        section {
            max-width: 1000px;
            margin: 40px auto;
            padding: 20px;
            background-color: #fff;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
            border-radius: 8px;
        }
        section h2 {
            font-size: 1.8rem;
            font-weight: 700;
            margin-bottom: 20px;
            color: #1d1d1f;
        }
        .contact {
            text-align: center;
            font-size: 1rem;
            margin-bottom: 40px;
        }
        .contact a {
            text-decoration: none;
            color: #007aff;
        }
        .projects {
            display: flex;
            flex-direction: column;
            gap: 20px;
        }
        .project {
            display: flex;
            gap: 20px;
            align-items: center;
            background-color: #f9f9f9;
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
            padding: 15px;
        }
        .project img {
            flex: 0 0 250px;
            width: 250px;
            height: 150px;
            object-fit: cover;
            border-radius: 8px;
        }
        .project-content {
            flex: 1;
        }
        .project h3 {
            font-size: 1.4rem;
            margin: 10px 0;
            font-weight: 700;
            color: #1d1d1f;
        }
        .project p {
            font-size: 1rem;
            margin-bottom: 10px;
            color: #555;
        }
        .project a {
            text-decoration: none;
            color: #007aff;
            font-weight: 500;
        }
        footer {
            text-align: center;
            padding: 20px 10px;
            font-size: 0.9rem;
            color: #777;
            background-color: #f5f5f7;
        }
    </style>
</head>
<body>
    <header>
        <div class="header-content">
            <h1>Pierre Vuillecard</h1>
            <p>PhD student | EPFL | IDIAP</p>
        </div>
        <div class="header-photo">
            <img src="https://raw.githubusercontent.com/Vuillecard/Vuillecard.github.io/refs/heads/main/images/profile_picture.jpeg" alt="[Your Name]'s photo">
        </div>
    </header>
    <section>
        <h3>About Me</h3>
        <p>I am a PhD student at EPFL (École Polytechnique Fédérale de Lausanne) and Idiap Research Institute in Switzerland, specializing in computer vision, deep learning,
             and human-centric scene understanding under the supervision of Jean-Marc Odobez. I hold an M.Sc. in Computational Science and a B.Sc. in Mathematics, both from EPFL.</p>
    </section>
    <section>
        <h3> Research Projects</h3>
        <div class="projects">
            <div class="project">
                <img src="https://raw.githubusercontent.com/Vuillecard/Vuillecard.github.io/refs/heads/main/images/gazeinteract.png" alt="Project 1">
                <div class="project-content">
                    <a href="https://openreview.net/forum?id=ALU676zGFE" target="_blank"><strong>A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction</strong></a>
                    <p style="font-size: 0.9rem; font-style: italic; color: #666; margin: 0;">Anshul Gupta, Samy Tafasca, Arya Farkhondeh, Pierre Vuillecard, Jean-Marc Odobez</p>
                    <p style="font-size: 0.9rem; font-style: italic; color: #666; margin: 0;">The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024</p>
                    <p>We introduce a novel framework to jointly predict the gaze target and social gaze label for all people in the scene. It comprises: (i) a temporal, transformer-based architecture that, in addition to image tokens, handles person-specific tokens capturing the gaze information related to each individual; (ii) a new dataset, VSGaze, that unifies annotation types across multiple gaze following and social gaze datasets. We show that our model trained on VSGaze can address all tasks jointly, and achieves state-of-the-art results for multi-person gaze following and social gaze prediction.</p>
                </div>
            </div>
            <div class="project">
                <img src="https://raw.githubusercontent.com/Vuillecard/Vuillecard.github.io/refs/heads/main/images/gazevlm.png" alt="Project 1">
                <div class="project-content">
                    <!-- <h3>Project 1</h3> -->
                    <a href="https://project2-link.com" target="_blank"><strong>Exploring the Zero-Shot Capabilities of VLMs for Improving Gaze Following</strong></a>
                    <p style="font-size: 0.9rem; color: #666; margin: 0;">Anshul Gupta*, Pierre Vuillecard*, Arya Farkhondeh, Jean-Marc Odobez</p>
                    <p style="font-size: 0.9rem; font-style: italic; color: #666; margin: 0;">IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR) Workshops, 2024</p>
                    <p style="font-size: 0.9rem; font-style: italic; color: #666; margin: 0;">Best Paper Award</p>
                    <p style="font-size: 0.9rem; margin: 0;"><a href="https://youtu.be/Vgrgi53efkE?t=4383" target="_blank">Video</a></p>
                    <p> We investigate the zero-shot capabilities of Vision-Language Models (VLMs) for extracting a wide array of contextual cues to improve gaze following performance. We first evaluate various VLMs, prompting strategies, and in-context learning (ICL) techniques for zero-shot cue recognition performance. Our analysis indicates that BLIP-2 is the overall top performing VLM and that ICL can improve performance. For gaze following, incorporating the extracted cues results in better generalization performance, especially when considering a larger set of cues.</p>
                </div>
            </div>
            <div class="project">
                <img src="https://raw.githubusercontent.com/idiap/ccdbhg-head-gesture-recognition/refs/heads/main/images/webapp_screen.png" alt="Project 1">
                <div class="project-content">
                    <!-- <h3>Project 1</h3> -->
                    <a href="https://github.com/idiap/ccdbhg-head-gesture-recognition" target="_blank"><strong>Demo Real-time Head Gesture Recognition</strong></a>
                    <p style="font-size: 0.9rem; margin: 0;"><a href="https://github.com/idiap/ccdbhg-head-gesture-recognition" target="_blank">Code</a></p>
                    <p> WebApp that transform your head gesture into a labeled gif and display them in real-time. It can handle up to 4 people at the same time.</p>
                </div>
            </div>
            <div class="project">
                <img src="https://raw.githubusercontent.com/Vuillecard/Vuillecard.github.io/refs/heads/main/images/All_gestures.png" alt="Project 2">
                <div class="project-content">
                    <!-- <h3>Project 2</h3> -->
                    <a href="https://publications.idiap.ch/attachments/papers/2024/Vuillecard_FG_2024.pdf" target="_blank"><strong>Novel Annotations and Gaze-Aware Representations for Head Gesture Recognition</strong></a>
                    <p style="font-size: 0.9rem; color: #666; margin: 0;">Pierre Vuillecard, Arya Farkhondeh, Michael Villamizar, and Jean-Marc Odobez</p>
                    <p style="font-size: 0.9rem; font-style: italic; color: #666; margin: 0;">18th IEEE Int. Conference on Automatic Face and Gesture Recognition (FG), 2024</p>
                    <p style="font-size: 0.9rem; margin: 0;"><a href="https://github.com/idiap/ccdbhg-head-gesture-recognition" target="_blank">Code</a> / <a href="https://zenodo.org/records/13927536" target="_blank">Data</a></p>
                    <p> CCDb-HG is a novel and large dataset for head gesture recognition, 
                        featuring diverse head gesture categories. Furthermore, we explore the inclusion of gaze as an auxiliary cue, 
                        along with geometric and temporal data augmentation, to enhance generalization. Additionally, we evaluate various model 
                        architectures to establish baseline performance on CCDb-HG.</p>
                        
                </div>
            </div>
            <!-- Add more project cards as needed -->
        </div>
    </section>
    <footer>
        &copy; 2024 Pierre Vuillecard. All Rights Reserved.
    </footer>
</body>
</html>
